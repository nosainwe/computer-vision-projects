# ğŸ§© Segment Anything Model (SAM) â€” Zeroâ€‘Shot Interactive Segmentation

A practical, notebook-based implementation of **Meta AIâ€™s Segment Anything Model (SAM)** â€” a promptable foundation model that can segment objects in images **without retraining**.

This project demonstrates how to use SAM with:
- âœ… **point prompts** (clicks)
- âœ… **bounding box prompts**
- âœ… **automatic mask generation**

---

## ğŸ“Œ Project overview

**Goal:** Perform **zeroâ€‘shot** image segmentation using user-provided prompts (points, boxes, or coarse masks).

**Why SAM is useful:**
- Works across many object types and environments
- Doesnâ€™t require task-specific training for basic segmentation
- Can be used as a â€œsegmentation backboneâ€ inside larger systems (labeling tools, AR, medical, robotics)

---

## ğŸ§  How SAM works (brief but accurate)

SAM has three main parts:

### 1) Image encoder
A Vision Transformer (ViT) processes the image and produces a dense feature map.

### 2) Prompt encoder
Encodes your prompt into embeddings:
- **Sparse prompts** (points, boxes) â†’ token embeddings
- **Dense prompts** (masks) â†’ feature-map style embeddings

### 3) Mask decoder
A lightweight transformer combines:
- image features
- prompt embeddings
- learnable output tokens

â€¦then produces:
- up to **3 candidate masks** per prompt
- an **IoU quality score** for each mask (to help pick the best one)

### Training (context)
SAM was trained on the **SAâ€‘1B** dataset (large-scale segmentation masks) using an annotation pipeline that moves from manual labeling toward automated mask generation.

---

## ğŸ—‚ï¸ Repository structure (suggested)

```text
sam-segmentation/
â”œâ”€â”€ sam_segmentation.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> If this lives inside a multi-project repo, keep it under:
> `computer-vision/sam-segmentation/`

---

## âš™ï¸ Setup

### 1) Create a virtual environment (recommended)

```bash
python -m venv .venv
# Windows:
# .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
```

### 2) Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Run the notebook

1. Launch Jupyter
2. Open: `sam_segmentation.ipynb`
3. Run all cells

The notebook should:
- load a preâ€‘trained SAM model
- load an image (sample or your own)
- run segmentation using:
  - point prompts
  - bounding box prompts
  - automatic mask generation
- visualise masks over the image

---

## ğŸ“Š Results (what to expect)

SAM typically produces strong masks with minimal prompting, especially for clear foreground objects.

> Replace this with your own example output (recommended):
- Save a sample to `assets/` and embed it in README.md
- Mention which prompt you used and the model type (e.g., ViTâ€‘B / ViTâ€‘L)

---

## ğŸ”§ Future improvements

- Fineâ€‘tune SAM for a domain (e.g., medical, satellite imagery).
- Combine SAM + text (e.g., CLIP) for text-guided segmentation.
- Export to ONNX for faster inference.
- Build an interactive demo with **Gradio** or **Streamlit**.

---

## ğŸ™ Acknowledgements

- Paper: **Segment Anything** â€” Kirillov et al. (2023)
- SAM model + ecosystem by Meta AI
- Inspiration: Mayank Pratap Singh and Sreedath Panat (article/tutorial)

---

## ğŸ“ Notes on originality (recommended)

If you learned from a tutorial/article, keep attribution (as above) and make your repo stronger by adding:
- clear setup steps,
- your own test images,
- a small CLI script (`predict.py`) so people can run SAM without opening Jupyter.
