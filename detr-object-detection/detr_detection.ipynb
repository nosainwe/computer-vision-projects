{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Detection Transformer (DETR)\n",
    "\n",
    "This notebook demonstrates how to use a pre‑trained DETR model from the HuggingFace `transformers` library to detect objects in images. We'll load an image, run inference, and visualize the predicted bounding boxes and labels.\n",
    "\n",
    "DETR (Detection Transformer) reframes object detection as a direct set prediction problem, eliminating the need for anchor boxes, region proposals, and non‑maximum suppression. It uses a CNN backbone to extract features, a transformer encoder–decoder to reason about objects globally, and a set‑based loss (Hungarian matching) during training.\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre‑trained DETR Model and Processor\n",
    "\n",
    "We'll use the `facebook/detr-resnet-50` model, which was trained on the COCO dataset. The processor handles image resizing, normalization, and conversion to tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load an Image\n",
    "\n",
    "We'll download a sample image from the web. You can also replace this with your own image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of a sample image (COCO style street scene)\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "print(\"Image size:\", image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "We process the image, pass it through the model, and obtain predictions. The model outputs `logits` (class scores) and `pred_boxes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Model outputs are in raw format; we need to post-process them to get final boxes and labels.\n",
    "target_sizes = torch.tensor([image.size[::-1]])  # (height, width)\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n",
    "\n",
    "print(f\"Detected {len(results['labels'])} objects with confidence > 0.7:\")\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"  {model.config.id2label[label.item()]}: {round(score.item(), 3)} at {box}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Detections\n",
    "\n",
    "We'll draw the bounding boxes and labels on the image using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(image, results, threshold=0.7):\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                 linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        caption = f\"{model.config.id2label[label.item()]}: {score:.2f}\"\n",
    "        ax.text(x1, y1, caption, fontsize=10, color='white',\n",
    "                bbox=dict(facecolor='green', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_detections(image, results, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How DETR Works – A Quick Recap\n",
    "\n",
    "DETR consists of four main parts:\n",
    "\n",
    "1. **CNN Backbone** (e.g., ResNet‑50) – extracts a feature map from the image.\n",
    "2. **Transformer Encoder** – the feature map is flattened, positional encodings are added, and a standard transformer encoder processes the sequence, allowing global context to be captured.\n",
    "3. **Transformer Decoder** – a fixed set of **object queries** (learned embeddings) interact with the encoder output via cross‑attention. The decoder outputs a representation for each query.\n",
    "4. **Prediction Heads** – two small feed‑forward networks per query: one for class probabilities (including a special “no object” class) and one for bounding box coordinates `(x, y, w, h)`.\n",
    "\n",
    "**Training** uses the Hungarian algorithm to match predictions to ground truth and a composite loss (classification + L1 box + GIoU). **Inference** simply takes the predictions with confidence above a threshold – no anchor boxes or NMS required.\n",
    "\n",
    "## 7. Next Steps\n",
    "\n",
    "- Try the model on your own images (replace the URL with a local path).\n",
    "- Fine‑tune the model on a custom dataset.\n",
    "- Explore the source code of DETR in the HuggingFace library.\n",
    "- Implement the Hungarian matching and loss yourself for deeper understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}